version: "3.9"

services:
  frontend:
    image: challenges-frontend:1.0
    environment:
      - SPRING_CLOUD_GATEWAY_HOST=gateway
    ports:
      - '3000:80'
    networks:
      - microservices
  multiplication:
    image: multiplication:0.0.1-SNAPSHOT
    environment:
      - SPRING_PROFILES_ACTIVE=docker
      - SPRING_CLOUD_NACOS_HOST=nacos
      - SPRING_CLOUD_ROCKETMQ_PROXY_HOST=proxy
      - OTEL_SERVICE_NAME=multiplication
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://collector:4318
      # Logs are disabled by default
      - OTEL_LOGS_EXPORTER=otlp
      - OTEL_TRACES_EXPORTER=otlp
      - OTEL_METRICS_EXPORTER=otlp
      # Optional specify file configuration instead of using environment variable scheme
      # To use, call "export OTEL_CONFIG_FILE=/sdk-config.yaml" before calling docker compose up
      - OTEL_CONFIG_FILE=
    depends_on:
      - namesrv
      - nacos
      - collector
    networks:
      - microservices
  gamification:
    image: gamification:0.0.1-SNAPSHOT
    environment:
      - SPRING_PROFILES_ACTIVE=docker
      - SPRING_CLOUD_NACOS_HOST=nacos
      - SPRING_CLOUD_ROCKETMQ_PROXY_HOST=proxy
      - OTEL_SERVICE_NAME=gamification
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://collector:4318
      # Logs are disabled by default
      - OTEL_LOGS_EXPORTER=otlp
      - OTEL_TRACES_EXPORTER=otlp
      - OTEL_METRICS_EXPORTER=otlp
      # Optional specify file configuration instead of using environment variable scheme
      # To use, call "export OTEL_CONFIG_FILE=/sdk-config.yaml" before calling docker compose up
      - OTEL_CONFIG_FILE=
    depends_on:
      - namesrv
      - nacos
      - collector
    networks:
      - microservices
  gateway:
    image: gateway:0.0.1-SNAPSHOT
    container_name: gateway
    ports:
      - '8000:8000'
    environment:
      - SPRING_PROFILES_ACTIVE=docker
      - SPRING_CLOUD_NACOS_HOST=nacos
      - SPRING_CLOUD_ROCKETMQ_PROXY_HOST=proxy
      - OTEL_SERVICE_NAME=gateway
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://collector:4318
      # Logs are disabled by default
      - OTEL_LOGS_EXPORTER=otlp
      - OTEL_TRACES_EXPORTER=otlp
      - OTEL_METRICS_EXPORTER=otlp
      # Optional specify file configuration instead of using environment variable scheme
      # To use, call "export OTEL_CONFIG_FILE=/sdk-config.yaml" before calling docker compose up
      - OTEL_CONFIG_FILE=
    depends_on:
      - namesrv
      - nacos
      - collector
    networks:
      - microservices
  logs:
    image: logs:0.0.1-SNAPSHOT
    environment:
      - SPRING_PROFILES_ACTIVE=docker
      - SPRING_CLOUD_NACOS_HOST=nacos
      - SPRING_CLOUD_ROCKETMQ_PROXY_HOST=proxy
      - OTEL_SERVICE_NAME=logs
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://collector:4318
      # Logs are disabled by default
      - OTEL_LOGS_EXPORTER=otlp
      - OTEL_TRACES_EXPORTER=otlp
      - OTEL_METRICS_EXPORTER=otlp
      # Optional specify file configuration instead of using environment variable scheme
      # To use, call "export OTEL_CONFIG_FILE=/sdk-config.yaml" before calling docker compose up
      - OTEL_CONFIG_FILE=
    depends_on:
      - namesrv
      - nacos
      - collector
    networks:
      - microservices
  namesrv:
    image: apache/rocketmq:5.3.3
    container_name: rmqnamesrv
    ports:
      - 9876:9876
    networks:
      - microservices
    command: sh mqnamesrv
  broker:
    image: apache/rocketmq:5.3.3
    container_name: rmqbroker
    ports:
      - 10909:10909
      - 10911:10911
      - 10912:10912
    environment:
      - NAMESRV_ADDR=rmqnamesrv:9876
    depends_on:
      - namesrv
    networks:
      - microservices
    volumes:
      - ./config/rocketmq/broker.conf:/home/rocketmq/rocketmq-5.3.3/conf/broker.conf
    command: sh mqbroker -n namesrv:9876 -c ../conf/broker.conf
  proxy:
    image: apache/rocketmq:5.3.3
    container_name: rmqproxy
    networks:
      - microservices
    depends_on:
      - broker
      - namesrv
    ports:
      - 8080:8080
      - 8081:8081
    restart: on-failure
    environment:
      - NAMESRV_ADDR=rmqnamesrv:9876
    command: sh mqproxy
  nacos:
    image: nacos/nacos-server:v3.0.1
    env_file:
      - ./env/nacos-standlone-mysql.env
    volumes:
      - ./standalone-logs/:/home/nacos/logs
    ports:
      #web console port
      - "8082:8080"
      - "8848:8848"
      - "9848:9848"
    depends_on:
      mysql:
        condition: service_healthy
    restart: always
    networks:
      - microservices
  mysql:
    container_name: mysql
    build:
      context: .
      dockerfile: ./image/mysql/8/Dockerfile
    image: example/mysql:8.0.30
    env_file:
      - ./env/mysql.env
    volumes:
      - ./mysql:/var/lib/mysql
    ports:
      - "3306:3306"
    healthcheck:
      test: [ "CMD", "mysqladmin" ,"ping", "-h", "localhost" ]
      interval: 5s
      timeout: 10s
      retries: 10
    networks:
      - microservices
  sentinel-dashboard:
    #image: bladex/sentinel-dashboard:latest
    image: sentinel-dashboard:v1.8.8
    container_name: sentinel-dashboard
    ports:
      - "8858:8080"
    restart: always
    networks:
      - microservices
  #for traces
  zipkin:
    image: 'openzipkin/zipkin'
    container_name: zipkin
    ports:
      - 9411:9411
    healthcheck:
      test: wget --no-verbose --tries=1 --spider http://localhost:9411/health || exit 1
      interval: 10s
      timeout: 10s
      retries: 3
    networks:
      - microservices
  collector:
    image: otel/opentelemetry-collector-contrib:0.128.0
    volumes:
      - ./collector-config.yaml:/collector-config.yaml
    command: ["--config=/collector-config.yaml"]
      #expose:
    #- "4318"
    ports:
      - "1888:1888"   # pprof extension
      - "8888:8888"   # Prometheus metrics exposed by the collector
      - "8889:8889"   # Prometheus exporter metrics
      - "13133:13133" # health_check extension
      - "4317:4317"   # OTLP gRPC receiver
      - "4318:4318"   # OTLP HTTP receiver
      - "55679:55679" # zpages extension
    networks:
      - microservices
  # Jaeger for traces
  jaeger:
    image: jaegertracing/all-in-one:latest
    container_name: jaeger
    command:
      - "--memory.max-traces=5000"
      - "--query.base-path=/jaeger/ui"
      - "--prometheus.server-url=http://prometheus:9090"
      - "--prometheus.query.normalize-calls=true"
      - "--prometheus.query.normalize-duration=true"
    deploy:
      resources:
        limits:
          memory: 400M
    restart: unless-stopped
    ports:
      - "16686:16686"
      - "14268"
      - "14250"
    environment:
      - METRICS_STORAGE_TYPE=prometheus
    networks:
      - microservices
  #currently for traces and logs
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:9.0.0
    container_name: elasticsearch
    environment:
      discovery.type: single-node
      ES_JAVA_OPTS: "-Xms1024m -Xmx1024m"
      ELASTIC_PASSWORD: changeme
      xpack.security.enabled: false
      xpack.license.self_generated.type: basic
      xpack.security.http.ssl.enabled: false
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    ports:
      - "9200:9200"
      - "9300:9300"
    healthcheck:
      test: ["CMD-SHELL", "curl --silent --fail --user elastic:changeme localhost:9200/_cluster/health || exit 1"]
      interval: 10s
      timeout: 30s
      retries: 3
    networks:
      - microservices
  kibana:
    depends_on:
      elasticsearch:
        condition: service_healthy
    image: docker.elastic.co/kibana/kibana:9.0.0
      #volumes:
    #- kibanadata:/usr/share/kibana/data
    ports:
      - 5601:5601
    environment:
      - SERVERNAME=kibana
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
      - ELASTICSEARCH_USERNAME=kibana_system
      - ELASTICSEARCH_PASSWORD=changeme
    mem_limit: 1073741824
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "curl -s -I http://localhost:5601 | grep -q 'HTTP/1.1 302 Found'",
        ]
      interval: 10s
      timeout: 10s
      retries: 120
    networks:
      - microservices
  # Grafana
  grafana:
    image: grafana/grafana:12.0.2
    #image: grafana/grafana:11.6.1
    container_name: grafana
    deploy:
      resources:
        limits:
          memory: 300M
    restart: unless-stopped
    environment:
      #- GF_INSTALL_PLUGINS=grafana-opensearch-datasource
      - GF_PATHS_PROVISIONING=/etc/grafana/provisioning
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin
    volumes:
      - ./config/grafana/grafana.ini:/etc/grafana/grafana.ini
      - ./config/grafana/provisioning/:/etc/grafana/provisioning/
    ports:
      - "3001:3000"
    depends_on:
      - prometheus
    networks:
      - microservices
  #loki
  loki:
    image: grafana/loki:3.1.0
    ports:
      - "3100:3100"
    command: -config.file=/etc/loki/local-config.yaml
    networks:
      - microservices
  tempo:
    image: grafana/tempo:latest
    container_name: tempo
    ports:
      - "3200:3200"
    command: [ "-config.file=/etc/tempo.yaml" ]
    volumes:
      - ./config/tempo/tempo.yaml:/etc/tempo.yaml
      - ./tempo-data:/var/tempo
    networks:
      - microservices
  # Prometheus
  prometheus:
    image: quay.io/prometheus/prometheus:v3.3.1
    container_name: prometheus
    command:
      - --web.console.templates=/etc/prometheus/consoles
      - --web.console.libraries=/etc/prometheus/console_libraries
      - --storage.tsdb.retention.time=1h
      - --config.file=/etc/prometheus/prometheus-config.yaml
      - --storage.tsdb.path=/prometheus
      - --web.enable-lifecycle
      - --web.route-prefix=/
      - --web.enable-otlp-receiver
      - --enable-feature=exemplar-storage
      - --enable-feature=native-histograms
      - --web.enable-remote-write-receiver
    volumes:
      - ./config/prometheus/prometheus-config.yaml:/etc/prometheus/prometheus-config.yaml
    deploy:
      resources:
        limits:
          memory: 300M
    restart: unless-stopped
    ports:
      - "9090:9090"
    networks:
      - microservices
networks:
  microservices:
    driver: bridge